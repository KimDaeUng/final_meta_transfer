{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "001_ARSC_TASKEMB.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "REm61TBTybks"
      ],
      "mount_file_id": "1Rtl_mGEI_ygJgUQ5rjVJPsNGFunQRK-z",
      "authorship_tag": "ABX9TyO9JrtIWix9gESW2AhNCWz4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KimDaeUng/final_meta_transfer/blob/master/001_ARSC_TASKEMB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SGAbstQk6lf"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SVmKD_6KpFM"
      },
      "source": [
        "from os import path\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bO9xRDoJigV"
      },
      "source": [
        "!git clone https://github.com/Gorov/DiverseFewShot_Amazon.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4g1OdYoKuBa"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers\n",
        "%cd transformers\n",
        "!pip install .\n",
        "!pip install -r ./examples/requirements.txt\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEl-Tb2gKLbE"
      },
      "source": [
        "!pip install pyarrow==1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoeAEKgmQ5zp"
      },
      "source": [
        "%cd '/content/transformers/examples/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeH8Bd7HQ9Eo"
      },
      "source": [
        "!git clone https://github.com/KimDaeUng/final_meta_transfer.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiKVvuTz-bi-"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import csv\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import collections\n",
        "import random\n",
        "import json, pickle\n",
        "import tqdm\n",
        "from torch.utils.data import TensorDataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGFuP15crtLu"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Jvk7Ax9ryMI"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfqI5AipOXUK"
      },
      "source": [
        "filelist = \"/content/DiverseFewShot_Amazon/Amazon_few_shot/workspace.filtered.list\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPi8K6okMcvP"
      },
      "source": [
        "def load_train_test_files(listfilename, test_suffix='.test'):\n",
        "    filein = open(listfilename, 'r')\n",
        "    file_tuples = []\n",
        "    for line in filein:\n",
        "        for k in ['2', '4','5']:\n",
        "          array = line.strip().split('\\t')\n",
        "          line = array[0]\n",
        "          trainfile = line + \".t{}\".format(k) + '.train'\n",
        "          devfile = line + \".t{}\".format(k) +  '.dev'\n",
        "          testfile = line + \".t{}\".format(k) +  test_suffix\n",
        "          file_tuples.append((trainfile, devfile, testfile))\n",
        "    filein.close()\n",
        "    return file_tuples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCXPtWm9MegJ"
      },
      "source": [
        "datasets = []\n",
        "list_dataset = []\n",
        "file_tuples = load_train_test_files(filelist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UyJPbjGVTz3"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJ8CfK4oPZsI"
      },
      "source": [
        "cd \"/content/DiverseFewShot_Amazon/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9wwh2CJPH5Y"
      },
      "source": [
        "workingdir = 'Amazon_few_shot'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p-Fz2ShieP7"
      },
      "source": [
        "def read_tsv(input_file):\n",
        "    with open(input_file, \"r\") as f:\n",
        "        texts, labels = [], []\n",
        "        while True:\n",
        "          line = f.readline()\n",
        "          if not line: break\n",
        "          text, label = line.strip().split('\\t')\n",
        "          texts.append(text)\n",
        "          labels.append(int(label) if label == '1' else 0)\n",
        "        return texts, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2njrMy2VXcq"
      },
      "source": [
        "datasets = {}\n",
        "for (trainfile, devfile, testfile) in tqdm.tqdm(file_tuples):\n",
        "    data_domain = trainfile.split(\".train\")[0]\n",
        "    data = {}\n",
        "    train_path = os.path.join(workingdir, trainfile)\n",
        "    dev_path = os.path.join(workingdir, devfile)\n",
        "    test_path = os.path.join(workingdir, testfile)\n",
        "    text, label = read_tsv(train_path)\n",
        "    data['train'] = {'text' : text, 'label' : label} \n",
        "    text, label = read_tsv(dev_path)\n",
        "    data['dev'] = {'text' : text, 'label' : label} \n",
        "    text, label = read_tsv(test_path)\n",
        "    data['test'] = {'text' : text, 'label' : label} \n",
        "    datasets[data_domain] = data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jrag-PohVlKi"
      },
      "source": [
        "datasets.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_otxwcCMkDLF"
      },
      "source": [
        "datasets['apparel.t2']['train']['text'][1], datasets['apparel.t2']['train']['label'][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuZp-5YZrnpC"
      },
      "source": [
        "## View Stats "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY-txLtikXuG"
      },
      "source": [
        "df_stat = {'domain' : [], 'train' : [], 'dev' : [], 'test' : []}\n",
        "for key, value in datasets.items():\n",
        "    df_stat['domain'].append(key)\n",
        "    for k, v in value.items():\n",
        "        df_stat[k].append(len(v['text']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EuirDxNo4Np"
      },
      "source": [
        "import pandas as pd\n",
        "df_s = pd.DataFrame(df_stat)\n",
        "pd.set_option('display.max_rows', 69)\n",
        "df_s['meta_split'] = df_s.domain.apply(lambda x : 'meta-test' if any([(i in x) for i in ['books', 'dvd', 'electronics', 'kitchen_housewares']]) else 'meta-train'  )\n",
        "df_s = df_s.sort_values(['meta_split', 'domain'], ascending=[False, True])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_w_tADlNpkYU"
      },
      "source": [
        "# Import seaborn library \n",
        "import seaborn as sns \n",
        "\n",
        "# Declaring the cm variable by the \n",
        "# color palette from seaborn \n",
        "cm = sns.light_palette(\"green\", as_cmap=True) \n",
        "\n",
        "# Visualizing the DataFrame with set precision \n",
        "print(\"\\nStatistics of the Dataset:\") \n",
        "df_s.style.background_gradient(cmap=cm).set_precision(2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unF08zuZMhdl"
      },
      "source": [
        "# Embedding and Visualizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfxWGG_rtiFg"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CiO4Gt8tZNV"
      },
      "source": [
        "!mkdir \"/content/transformers/examples/final_meta_transfer/data/model\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV2ugX2JsLaU"
      },
      "source": [
        "# Set BERT tokenizer\n",
        "from transformers import BertTokenizer\n",
        "path_model = \"/content/transformers/examples/final_meta_transfer/data/model\"\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True, cache_dir=path_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REm61TBTybks"
      },
      "source": [
        "#### nlp library "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ygt7t_Hw7ho"
      },
      "source": [
        "import nlp\n",
        "from datasets import Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axT7Y2wW3sd8"
      },
      "source": [
        "df_nlp = {}\n",
        "for key, value in datasets.items():\n",
        "    df_nlp_inner = {}\n",
        "    for split, textnlabel_dict in value.items():\n",
        "        df_nlp_inner[split] = Dataset.from_dict(textnlabel_dict)\n",
        "    df_nlp[key] = df_nlp_inner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H11YsxgkNovP"
      },
      "source": [
        "df_nlp['apparel.t2']['train']['label'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJVgaTFlygk7"
      },
      "source": [
        "#### Basic Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7p2T-iuiA1ky"
      },
      "source": [
        "# Create dataset  \n",
        "def create_feature_set(examples, tokenizer, max_seq_length=512):\n",
        "    input_len = len(examples['text'])\n",
        "    all_input_ids      = torch.empty(input_len, max_seq_length, dtype = torch.long)\n",
        "    all_attention_mask = torch.empty(input_len, max_seq_length, dtype = torch.long)\n",
        "    all_segment_ids    = torch.empty(input_len, max_seq_length, dtype = torch.long)\n",
        "    all_lengths    = torch.empty(input_len, dtype = torch.long)\n",
        "    all_label_ids      = torch.empty(input_len, dtype = torch.long)\n",
        "\n",
        "    for id_,example in enumerate(zip(examples['text'], examples['label'])):\n",
        "        input_ids = tokenizer.encode(example[0], max_length=max_seq_length, truncation=True)\n",
        "        if len(input_ids) > max_seq_length:\n",
        "            print(\"input_ids exceeds max_seq_length : {} > {}\".format(len(input_ids), max_seq_length))\n",
        "            input_ids = input_ids[:max_seq_length-1] + [input_ids[-1]]\n",
        "\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "        segment_ids    = [0] * len(input_ids)\n",
        "\n",
        "        while len(input_ids) < max_seq_length:\n",
        "            input_ids.append(0)\n",
        "            attention_mask.append(0)\n",
        "            segment_ids.append(0)\n",
        "\n",
        "        label_id = example[1]\n",
        "        all_input_ids[id_] = torch.Tensor(input_ids).to(torch.long)\n",
        "        all_attention_mask[id_] = torch.Tensor(attention_mask).to(torch.long)\n",
        "        all_segment_ids[id_] = torch.Tensor(segment_ids).to(torch.long)\n",
        "        all_label_ids[id_] = torch.Tensor([label_id]).to(torch.long)\n",
        "        all_lengths[id_] = torch.Tensor([sum(attention_mask)]).to(torch.long)\n",
        "\n",
        "    tensor_set = TensorDataset(all_input_ids, all_attention_mask, all_segment_ids, all_lengths, all_label_ids)  \n",
        "    return tensor_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IecwFVJkNzZe"
      },
      "source": [
        "apparel_t2 = create_feature_set(datasets['apparel.t2']['train'], tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pCIl7XH9xR6"
      },
      "source": [
        "apparel_t2[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAHKKv4g4PzO"
      },
      "source": [
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
        "                              TensorDataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0fICrjh48it"
      },
      "source": [
        "data_loader = DataLoader(apparel_t2,  batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nlvxn0Kq4pte"
      },
      "source": [
        "!mkdir \"/content/drive/My Drive/Colab Notebooks/Final/meta_transfer/data/amazon_emb\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4ltCAB04pti"
      },
      "source": [
        "# get path to save embedding tensor file(*.pt)\n",
        "emb_path = \"/content/drive/My Drive/Colab Notebooks/Final/meta_transfer/data/amazon_emb\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8y-_lkv4cE4"
      },
      "source": [
        "# Load pretrained model\n",
        "from transformers import BertModel\n",
        "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True) \n",
        "model.to('cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZF7hk0I4tnq"
      },
      "source": [
        "def save_emb(model, data_loader, task_name, split):\n",
        "    model.eval()\n",
        "    for i, batch in tqdm.tqdm(enumerate(data_loader)):\n",
        "        inp, seg, att, leng, label = batch\n",
        "        i = i+1\n",
        "        if i != len(data_loader):\n",
        "            i = i * data_loader.batch_size\n",
        "        else:\n",
        "            i = data_loader.batch_size*(len(data_loader)) + len(data_loader.dataset) % len(data_loader)\n",
        "        \n",
        "        save_path = os.path.join(emb_path,\"{}_{}_{}.pt\".format(task_name, split, str(i).zfill(5)))\n",
        "        print(save_path)\n",
        "        if os.path.isfile(save_path):\n",
        "            continue\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                hidden = model(inp.cuda(), seg.cuda(), att.cuda())[2]\n",
        "                token= hidden[-1].to(\"cpu\")\n",
        "                sentence = torch.mean(token, dim=1)\n",
        "                bat = {\"sentence\" : sentence, \"length\":leng, 'label' : label}\n",
        "                torch.save(bat, save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ_2aZMd_rd1"
      },
      "source": [
        "def get_taskemb(model, data_loader, task_name, split):\n",
        "    model.eval()\n",
        "\n",
        "    task_emb = torch.empty(768)\n",
        "    task_norm = torch.empty(1)\n",
        "    for i, batch in tqdm.tqdm(enumerate(data_loader)):\n",
        "        inp, seg, att, leng, label = batch\n",
        "        i = i+1\n",
        "        if i != len(data_loader):\n",
        "            i = i * data_loader.batch_size\n",
        "        else:\n",
        "            i = data_loader.batch_size*(len(data_loader)) + len(data_loader.dataset) % len(data_loader)\n",
        "        \n",
        "        print(\"{}_{}_{}\".format(task_name, split, str(i).zfill(3)))\n",
        "        with torch.no_grad():\n",
        "            hidden = model(inp.cuda(), seg.cuda(), att.cuda())[2]\n",
        "            hx = hidden[-1].to(\"cpu\").detach()\n",
        "            hx = torch.mean(hx, dim=1)\n",
        "\n",
        "            task_norm += torch.sum(torch.square(hx))\n",
        "            # Batch-wise summation\n",
        "            hx = torch.sum(hx, dim=0)\n",
        "            task_emb += hx\n",
        "\n",
        "    task_norm = torch.sqrt(task_norm)\n",
        "    \n",
        "    return task_emb/task_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5JsFP5WGaQ6"
      },
      "source": [
        "### Get TASKEMBs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cir7HPPVP-yi"
      },
      "source": [
        "# [TO DO] add dev sets to this\n",
        "task_emb_dic = {}\n",
        "for task, corpus in datasets.items():\n",
        "    print(\"-\"*50)\n",
        "    print(\"Task : \", task)\n",
        "    encoded_corpus = create_feature_set(corpus['train'], tokenizer)\n",
        "    data_loader = DataLoader(encoded_corpus,  batch_size=128)\n",
        "    task_emb = get_taskemb(model, data_loader, task, 'train')\n",
        "    task_emb_dic[task] = task_emb\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsMCLV7VSc7_"
      },
      "source": [
        "# Save Task Embedding\n",
        "task_emb_path = '/content/drive/My Drive/Colab Notebooks/Final/meta_transfer/data/task_emb_dic.pt'\n",
        "torch.save(task_emb_dic, task_emb_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4IK-dqGUX-6"
      },
      "source": [
        "task_emb_dic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyRBJouLUSpo"
      },
      "source": [
        "# Below Codes for Meta-Training Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10b1XdGpBU8Q"
      },
      "source": [
        "# # For whole dataset, preprocess (Not Recommanded)\n",
        "# df_nlp_process = {}\n",
        "# for key, value in df_nlp.items():\n",
        "#     print(\"Task : \", key)\n",
        "#     df_nlp_process_inner = {}\n",
        "#     for split, dset in value.items():\n",
        "#         print(\"\\t\", split)\n",
        "#         df_nlp_process_inner[split] = dset.map(\n",
        "#         lambda x: tokenizer(x['text'], padding=True,\n",
        "#                             return_length=True),batched=True)\n",
        "#     df_nlp_process[key] = df_nlp_process_inner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnBl56QiMb9Y"
      },
      "source": [
        "LABEL_MAP  = {'positive':0, 'negative':1, 0:'positive', 1:'negative'}\n",
        "\n",
        "class MetaTask(Dataset):\n",
        "    \n",
        "    def __init__(self, examples, num_task, k_support, k_query, tokenizer):\n",
        "        \"\"\"\n",
        "        :param samples: list of samples\n",
        "        :param num_task: number of training tasks.\n",
        "        :param k_support: number of support sample per task\n",
        "        :param k_query: number of query sample per task\n",
        "        \"\"\"\n",
        "        self.examples = examples\n",
        "        random.shuffle(self.examples)\n",
        "        \n",
        "        self.num_task = num_task\n",
        "        self.k_support = k_support\n",
        "        self.k_query = k_query\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_seq_length = 512\n",
        "        self.create_batch(self.num_task)\n",
        "    \n",
        "    def create_batch(self, num_task):\n",
        "        self.supports = []  # support set\n",
        "        self.queries = []  # query set\n",
        "        \n",
        "        for b in range(num_task):  # for each task\n",
        "            # 1.select domain randomly\n",
        "            domain = random.choice(self.examples.keys())\n",
        "            domainExamples = [e for e in self.examples.items() if domain in e[0] ]\n",
        "            \n",
        "            # 1.select k_support + k_query examples from domain randomly\n",
        "            selected_examples = random.sample(domainExamples,self.k_support + self.k_query)\n",
        "            random.shuffle(selected_examples)\n",
        "            exam_train = selected_examples[:self.k_support]\n",
        "            exam_test  = selected_examples[self.k_support:]\n",
        "            \n",
        "            self.supports.append(exam_train)\n",
        "            self.queries.append(exam_test)\n",
        "\n",
        "    def create_feature_set(self,examples):\n",
        "        all_input_ids      = torch.empty(len(examples), self.max_seq_length, dtype = torch.long)\n",
        "        all_attention_mask = torch.empty(len(examples), self.max_seq_length, dtype = torch.long)\n",
        "        all_segment_ids    = torch.empty(len(examples), self.max_seq_length, dtype = torch.long)\n",
        "        all_label_ids      = torch.empty(len(examples), dtype = torch.long)\n",
        "\n",
        "        for id_,example in enumerate(examples):\n",
        "            input_ids = self.tokenizer.encode(example['text'])\n",
        "            attention_mask = [1] * len(input_ids)\n",
        "            segment_ids    = [0] * len(input_ids)\n",
        "\n",
        "            while len(input_ids) < self.max_seq_length:\n",
        "                input_ids.append(0)\n",
        "                attention_mask.append(0)\n",
        "                segment_ids.append(0)\n",
        "\n",
        "            label_id = LABEL_MAP[example['label']]\n",
        "            all_input_ids[id_] = torch.Tensor(input_ids).to(torch.long)\n",
        "            all_attention_mask[id_] = torch.Tensor(attention_mask).to(torch.long)\n",
        "            all_segment_ids[id_] = torch.Tensor(segment_ids).to(torch.long)\n",
        "            all_label_ids[id_] = torch.Tensor([label_id]).to(torch.long)\n",
        "\n",
        "        tensor_set = TensorDataset(all_input_ids, all_attention_mask, all_segment_ids, all_label_ids)  \n",
        "        return tensor_set\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        support_set = self.create_feature_set(self.supports[index])\n",
        "        query_set   = self.create_feature_set(self.queries[index])\n",
        "        return support_set, query_set\n",
        "\n",
        "    def __len__(self):\n",
        "        # as we have built up to batchsz of sets, you can sample some small batch size of sets.\n",
        "        return self.num_task"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otiHGMOd-sRv"
      },
      "source": [
        "def get_train_examples(self, data_dir):\n",
        "    \"\"\"See base class.\"\"\"\n",
        "    logger.info(\"LOOKING AT {} train\".format(data_dir))\n",
        "    return self._create_examples(self._read_csv(data_dir)), \"train\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkA5lu0C-u8k"
      },
      "source": [
        "def _read_tsv(input_file):\n",
        "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        return list(csv.reader(f, delimiter='\\t'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijNmEjW8-ybX"
      },
      "source": [
        "data_kit = _read_tsv(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opdx36Dl_FPJ"
      },
      "source": [
        "data_kit[:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8vI6a8j9-LC"
      },
      "source": [
        "def _create_examples(lines, type):\n",
        "    \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "    if type == \"train\" and lines[0][-1] != \"label\":\n",
        "        raise ValueError(\"For training, the input file must contain a label column.\")\n",
        "\n",
        "    examples = [\n",
        "        InputExample(\n",
        "            example_id=id,\n",
        "            question=line[5],  # in the swag dataset, the\n",
        "            # common beginning of each\n",
        "            # choice is stored in \"sent2\".\n",
        "            contexts=[line[4], line[4], line[4], line[4]],\n",
        "            endings=[line[7], line[8], line[9], line[10]],\n",
        "            label=line[11],\n",
        "        )\n",
        "        for id, line in enumerate(lines)  # we skip the line with the column names\n",
        "    ]\n",
        "\n",
        "    return examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRuQGt8xcLy8"
      },
      "source": [
        "cd DiverseFewShot_Amazon/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKSHwa0QdDR3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}